{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd066da0-7f17-4dec-b228-6e9493d50e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d986f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e3295f-86c3-4f19-b573-84cdbec2f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_into_frames(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Read the video frame by frame\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if we reach the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image\n",
    "        frame_count += 1\n",
    "        frame_filename = f\"{output_folder}/frame_{frame_count:02d}.jpg\"\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Frames extracted and saved to {output_folder}\")\n",
    "\n",
    "\n",
    "def merge_close_contours(contours, distance_threshold):\n",
    "    \"\"\" Merge contours that are close to each other based on distance threshold. \"\"\"\n",
    "    centers = [np.mean(contour, axis=0)[0] for contour in contours]\n",
    "    merged_contours = []\n",
    "    used = [False] * len(contours)\n",
    "    \n",
    "    for i, center in enumerate(centers):\n",
    "        if not used[i]:\n",
    "            used[i] = True\n",
    "            merged_contour = [contours[i]]\n",
    "            for j, other_center in enumerate(centers):\n",
    "                if not used[j] and np.linalg.norm(center - other_center) < distance_threshold:\n",
    "                    used[j] = True\n",
    "                    merged_contour.append(contours[j])\n",
    "            merged_contours.append(np.vstack(merged_contour))\n",
    "    \n",
    "    return merged_contours\n",
    "\n",
    "def numeric_sort(file_path):\n",
    "    \"\"\" Helper function to extract numeric part for sorting \"\"\"\n",
    "    parts = file_path.split('_')\n",
    "    numeric_part = int(parts[-1].split('.')[0])  # Assumes format 'frame_01.jpg'\n",
    "    return numeric_part\n",
    "\n",
    "\n",
    "# Function to track green dots and calculate properties with subtraction of red channel\n",
    "def calculate_mean_pixels_with_tracking_sub(path_to_frames, output_csv_path, min_max_value,thresh,output_annotation):\n",
    "    jpg_files = glob.glob(path_to_frames + '/*.jpg')\n",
    "    jpg_files.sort(key=numeric_sort)  # Sort files numerically by frame number\n",
    "\n",
    "    stage = path_to_frames.split(os.sep)[-2]\n",
    "    video_num = path_to_frames.split(os.sep)[-1]\n",
    "\n",
    "    with open(output_csv_path, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        frame_idx = 1\n",
    "        dot_id = 1\n",
    "        dot_id_map = defaultdict(lambda: None)\n",
    "        \n",
    "        output_folder = os.path.join(output_annotation+'/annotated_frames_thresh_'+thresh, stage, video_num)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        distance_threshold = 20  # Adjust this based on your specific needs\n",
    "        \n",
    "        for file in jpg_files:\n",
    "            frame = cv2.imread(file)\n",
    "            if frame is None:\n",
    "                print(\"Error: Could not read the image.\")\n",
    "                continue\n",
    "            \n",
    "            # Adjust and process the frame\n",
    "            blue_channel, green_channel, red_channel = frame[:,:,0], frame[:,:,1], frame[:,:,2]\n",
    "            mask = red_channel > int(thresh)\n",
    "            avg_green_value = np.mean(green_channel[mask])\n",
    "            adjusted_green_channel = np.maximum(green_channel - avg_green_value, 0).astype(green_channel.dtype)\n",
    "            frame[:, :, 1] = adjusted_green_channel\n",
    "\n",
    "            _, binary_image = cv2.threshold(adjusted_green_channel, int(thresh), 255, cv2.THRESH_BINARY)\n",
    "            min_contour_area = 5\n",
    "            contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > min_contour_area]\n",
    "            merged_contours = merge_close_contours(filtered_contours, distance_threshold)\n",
    "            \n",
    "            # Create a mask for the green dots\n",
    "            mask = np.ones_like(adjusted_green_channel, dtype=np.uint8) * 255\n",
    "            for contour in merged_contours:\n",
    "                cv2.drawContours(mask, [contour], -1, 0, -1)  # Fill the contour area with black\n",
    "\n",
    "            # Calculate the average pixel value excluding the green dots for each color channel\n",
    "            avg_pixel_value_no_dots_b = cv2.mean(frame[:, :, 0], mask=mask)[0]\n",
    "            avg_pixel_value_no_dots_g = cv2.mean(frame[:, :, 1], mask=mask)[0]\n",
    "            avg_pixel_value_no_dots_r = cv2.mean(frame[:, :, 2], mask=mask)[0]\n",
    "\n",
    "            new_dot_id_map = {}\n",
    "            for contour in merged_contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                roi = frame[y:y+h, x:x+w]\n",
    "                mean_pixel_value = cv2.mean(roi)[1]\n",
    "                dot_size = cv2.contourArea(contour)\n",
    "                normalized_mean = (mean_pixel_value - min_max_value[0]) / (min_max_value[1] - min_max_value[0])\n",
    "                green_pixel_count = np.sum(mask[y:y+h, x:x+w] == 255)\n",
    "                pixel_sum = np.sum(roi[:, :, 1])\n",
    "\n",
    "                center = (x + w // 2, y + h // 2)\n",
    "                # Improved ID assignment logic to avoid close but distinct dots getting the same ID\n",
    "                closest_dot_id = None\n",
    "                closest_distance = float('inf')\n",
    "                for prev_id, prev_center in dot_id_map.items():\n",
    "                    if prev_center is not None:\n",
    "                        distance = np.linalg.norm(np.array(center) - np.array(prev_center))\n",
    "                        if distance < closest_distance:\n",
    "                            closest_dot_id = prev_id\n",
    "                            closest_distance = distance\n",
    "\n",
    "                if closest_distance < 50:  # Lower threshold for same dot consideration\n",
    "                    new_dot_id_map[closest_dot_id] = center\n",
    "                    dot_id_to_use = closest_dot_id\n",
    "                else:\n",
    "                    new_dot_id_map[dot_id] = center\n",
    "                    dot_id_to_use = dot_id\n",
    "                    dot_id += 1\n",
    "                \n",
    "                text_position = (center[0] + 30, center[1])\n",
    "                cv2.drawContours(frame, [contour], -1, (255, 255, 255), 2)\n",
    "                cv2.putText(frame, str(dot_id_to_use), text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)  # Text in white\n",
    "                csv_writer.writerow([video_num, stage, frame_idx, dot_id_to_use, x, y, mean_pixel_value, dot_size, normalized_mean, green_pixel_count, pixel_sum, avg_pixel_value_no_dots_b, avg_pixel_value_no_dots_g, avg_pixel_value_no_dots_r])\n",
    "\n",
    "            dot_id_map.update(new_dot_id_map)\n",
    "            frame_idx += 1\n",
    "            \n",
    "            annotated_frame_path = os.path.join(output_folder, f'frame_{frame_idx-1:04d}.jpg')\n",
    "            cv2.imwrite(annotated_frame_path, frame)\n",
    "            \n",
    "    print('Processing complete.')\n",
    "    \n",
    "    \n",
    "# Function to track green dots and calculate properties with no subtraction of red channel\n",
    "def calculate_mean_pixels_with_tracking_no_sub(path_to_frames, output_csv_path, min_max_value, thresh,output_annotation):\n",
    "    jpg_files = glob.glob(path_to_frames + '/*.jpg')\n",
    "    jpg_files.sort(key=numeric_sort)  # Sort files numerically by frame number\n",
    "\n",
    "    stage = path_to_frames.split(os.sep)[-2]\n",
    "    video_num = path_to_frames.split(os.sep)[-1]\n",
    "\n",
    "    with open(output_csv_path, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        frame_idx = 1\n",
    "        dot_id = 1\n",
    "        dot_id_map = defaultdict(lambda: None)  # Persistent mapping of dot IDs\n",
    "        \n",
    "        output_folder = os.path.join(output_annotation+'/annotated_frames_thresh_'+thresh, stage, video_num)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        distance_threshold = 20  # Adjust this based on your specific needs\n",
    "        \n",
    "        for file in jpg_files:\n",
    "            frame = cv2.imread(file)\n",
    "            if frame is None:\n",
    "                print(\"Error: Could not read the image.\")\n",
    "                continue\n",
    "            img = frame[:, :, 1]\n",
    "\n",
    "            _, binary_image = cv2.threshold(img, int(thresh), 255, cv2.THRESH_BINARY)\n",
    "            min_contour_area = 5\n",
    "            contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > min_contour_area]\n",
    "            merged_contours = merge_close_contours(filtered_contours, distance_threshold)\n",
    "            \n",
    "            # Create a mask for the green dots\n",
    "            mask = np.ones_like(img, dtype=np.uint8) * 255\n",
    "            for contour in merged_contours:\n",
    "                cv2.drawContours(mask, [contour], -1, 0, -1)  # Fill the contour area with black\n",
    "\n",
    "            # Calculate the average pixel value excluding the green dots for each color channel\n",
    "            avg_pixel_value_no_dots_b = cv2.mean(frame[:, :, 0], mask=mask)[0]\n",
    "            avg_pixel_value_no_dots_g = cv2.mean(frame[:, :, 1], mask=mask)[0]\n",
    "            avg_pixel_value_no_dots_r = cv2.mean(frame[:, :, 2], mask=mask)[0]\n",
    "\n",
    "            new_dot_id_map = {}\n",
    "            for contour in merged_contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                roi = frame[y:y+h, x:x+w]\n",
    "                mean_pixel_value = cv2.mean(roi)[1]\n",
    "                dot_size = cv2.contourArea(contour)\n",
    "                normalized_mean = (mean_pixel_value - min_max_value[0]) / (min_max_value[1] - min_max_value[0])\n",
    "                green_pixel_count = np.sum(mask[y:y+h, x:x+w] == 255)\n",
    "                pixel_sum = np.sum(roi[:, :, 1])\n",
    "\n",
    "                center = (x + w // 2, y + h // 2)\n",
    "                #logic to avoid close but distinct dots getting the same ID\n",
    "                closest_dot_id = None\n",
    "                closest_distance = float('inf')\n",
    "                for prev_id, prev_center in dot_id_map.items():\n",
    "                    if prev_center is not None:\n",
    "                        distance = np.linalg.norm(np.array(center) - np.array(prev_center))\n",
    "                        if distance < closest_distance:\n",
    "                            closest_dot_id = prev_id\n",
    "                            closest_distance = distance\n",
    "\n",
    "                if closest_distance < 50:  # Lower threshold for same dot consideration\n",
    "                    new_dot_id_map[closest_dot_id] = center\n",
    "                    dot_id_to_use = closest_dot_id\n",
    "                else:\n",
    "                    new_dot_id_map[dot_id] = center\n",
    "                    dot_id_to_use = dot_id\n",
    "                    dot_id += 1\n",
    "                text_position = (center[0] + 30, center[1])\n",
    "                cv2.drawContours(frame, [contour], -1, (255, 255, 255), 2)\n",
    "                cv2.putText(frame, str(dot_id_to_use), text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)  # Text in white\n",
    "\n",
    "                csv_writer.writerow([video_num, stage, frame_idx, dot_id_to_use, x, y, mean_pixel_value, dot_size, normalized_mean, green_pixel_count, pixel_sum, avg_pixel_value_no_dots_b, avg_pixel_value_no_dots_g, avg_pixel_value_no_dots_r])\n",
    "\n",
    "            dot_id_map.update(new_dot_id_map)\n",
    "            frame_idx += 1\n",
    "            \n",
    "            annotated_frame_path = os.path.join(output_folder, f'frame_{frame_idx-1:04d}.jpg')\n",
    "            cv2.imwrite(annotated_frame_path, frame)\n",
    "            \n",
    "    print('Processing complete.')    \n",
    "    \n",
    "\n",
    "def find_min_max_pixel_value_per_stage(path_to_frames,thresh):\n",
    "    min_brightness = np.inf\n",
    "    max_brightness = -np.inf\n",
    "    frame = 1  \n",
    "    jpg_files = glob.glob(path_to_frames + '/*.jpg')\n",
    "    stage = path_to_frames.split('\\\\')[-2]\n",
    "    video_num = path_to_frames.split('\\\\')[-1]\n",
    "    \n",
    "    for files in jpg_files:\n",
    "        contour_index = 0\n",
    "        image = cv2.imread(files)\n",
    "        img = image[:, :, 1]\n",
    "    \n",
    "        if image is None:\n",
    "            print(\"Error: Could not read the image.\")\n",
    "        else:\n",
    "            # Threshold the image to create a binary image\n",
    "            _, binary_image = cv2.threshold(img, int(thresh), 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "            # Find contours in the binary image\n",
    "            contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "            # Filter out small contours (assuming small contours correspond to noise)\n",
    "            min_contour_area = 5\n",
    "            filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > min_contour_area]\n",
    "    \n",
    "            # Calculate the average pixel value for each contour\n",
    "            for contour in filtered_contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                roi = image[y:y+h, x:x+w]\n",
    "                brightness = cv2.mean(roi)[1]\n",
    "                if brightness < min_brightness:\n",
    "                    min_brightness = brightness\n",
    "                if brightness > max_brightness:\n",
    "                    max_brightness = brightness\n",
    "                contour_index +=1\n",
    "        frame += 1\n",
    "    return (min_brightness,max_brightness)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fd392-75d2-44fa-8861-deb48bd2c121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c13e683d",
   "metadata": {},
   "source": [
    "### Getting frames out of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e68e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted and saved to ./data/sog_d/nc11/1\n",
      "Frames extracted and saved to ./data/sog_d/nc12/1\n",
      "Frames extracted and saved to ./data/sog_d/nc12/2\n",
      "Frames extracted and saved to ./data/sog_d/nc12/3\n",
      "Frames extracted and saved to ./data/sog_d/nc12/4\n",
      "Frames extracted and saved to ./data/sog_d/nc13/1\n",
      "Frames extracted and saved to ./data/sog_d/nc13/2\n",
      "Frames extracted and saved to ./data/sog_d/nc13/3\n",
      "Frames extracted and saved to ./data/sog_d/nc13/4\n",
      "Frames extracted and saved to ./data/sog_d/nc14/1\n",
      "Frames extracted and saved to ./data/sog_d/nc14/2\n",
      "Frames extracted and saved to ./data/sog_d/nc14/3\n",
      "Frames extracted and saved to ./data/sog_d/nc14/4\n",
      "Frames extracted and saved to ./data/sog_d/nc14/5\n",
      "Frames extracted and saved to ./data/sog_d/nc14/6\n",
      "Frames extracted and saved to ./data/sog_d/nc14/7\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc12/1\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc12/2\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc12/3\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc12/4\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc12/5\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/1\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/2\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/3\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/4\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/5\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc13/6\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/1\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/2\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/3\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/4\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/5\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/6\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/7\n",
      "Frames extracted and saved to ./data/sog_d_su_h/nc14/8\n"
     ]
    }
   ],
   "source": [
    "# File should be in a format of videos/gene_type/stage_gene_videonum.avi\n",
    "for gene in glob.glob(\"videos/*\"):\n",
    "    gene_type = gene.split('\\\\')\n",
    "    for video_path in glob.glob(\"videos/\"+gene_type[-1]+\"/*\"):\n",
    "        splits_file = video_path.split('\\\\')[-1].split('.')[-2].split('_')\n",
    "        split_video_into_frames(video_path,\"./data/\"+gene_type[-1]+\"/\"+splits_file[0]+\"/\"+splits_file[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b811c",
   "metadata": {},
   "source": [
    "### Getting tracking data and dataset with subtraction of red channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3397f8b6-451c-497b-b434-0a0f0fe12059",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = '55'\n",
    "min_max_brightness = []\n",
    "gene_type = \"sog_d\" #change this for different genes\n",
    "for stage in glob.glob(\"data/\"+gene_type+\"/*\"):\n",
    "    stage_min = np.inf\n",
    "    stage_max = -np.inf\n",
    "    for video_path in glob.glob(stage+\"\\*\"):\n",
    "        _min,_max = find_min_max_pixel_value_per_stage(video_path,thresh)\n",
    "        if _min < stage_min:\n",
    "            stage_min = _min\n",
    "        if _max > stage_max:\n",
    "            stage_max = _max\n",
    "    min_max_brightness.append((stage_min,stage_max))\n",
    "\n",
    "\n",
    "stage_counter = 0\n",
    "output_csv_path = \"result/\"+gene_type+\"/avg_green_dot_sub/annotated_frames_thresh_\"+thresh+\"_\"+gene_type+\"_mean_pixel_normalized_tracking.csv\"\n",
    "if not os.path.exists(\"result/\"+gene_type+\"/avg_green_dot_sub/annotated_frames_thresh_\"+thresh):\n",
    "            os.makedirs(\"result/\"+gene_type+\"/avg_green_dot_sub/annotated_frames_thresh_\"+thresh)\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['VideoNum', 'Stage', 'Frame', 'DotID','X', 'Y','MeanPixelValue', 'DotSize', 'NormalizedMean', 'GreenPixelCount', 'PixelSum','AvgPixelValueNoDots_B', 'AvgPixelValueNoDots_G', 'AvgPixelValueNoDots_R'])\n",
    "for frame_folder in glob.glob(\"data/\"+gene_type+\"/*\"):\n",
    "    for video in  glob.glob(frame_folder+\"/*\"):\n",
    "        calculate_mean_pixels_with_tracking_sub(video,output_csv_path,min_max_brightness[stage_counter],thresh,\"result/\"+gene_type+\"/avg_green_dot_sub\")\n",
    "    stage_counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2da3cf",
   "metadata": {},
   "source": [
    "### Getting tracking data and dataset with no subtraction of red channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddde897a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = '55'\n",
    "min_max_brightness = []\n",
    "gene_type = \"sog_d\" #change this for different genes\n",
    "for stage in glob.glob(\"data/\"+gene_type+\"/*\"):\n",
    "    stage_min = np.inf\n",
    "    stage_max = -np.inf\n",
    "    for video_path in glob.glob(stage+\"\\*\"):\n",
    "        _min,_max = find_min_max_pixel_value_per_stage(video_path,thresh)\n",
    "        if _min < stage_min:\n",
    "            stage_min = _min\n",
    "        if _max > stage_max:\n",
    "            stage_max = _max\n",
    "    min_max_brightness.append((stage_min,stage_max))\n",
    "\n",
    "stage_counter = 0\n",
    "output_csv_path = \"result/\"+gene_type+\"/avg_green_dot_no_sub/annotated_frames_thresh_\"+thresh+\"_\"+gene_type+\"_mean_pixel_normalized_pixel_tracking.csv\"\n",
    "if not os.path.exists(\"result/\"+gene_type+\"/avg_green_dot_no_sub/annotated_frames_thresh_\"+thresh):\n",
    "            os.makedirs(\"result/\"+gene_type+\"/avg_green_dot_no_sub/annotated_frames_thresh_\"+thresh)\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['VideoNum', 'Stage', 'Frame', 'DotID','X', 'Y','MeanPixelValue', 'DotSize', 'NormalizedMean', 'GreenPixelCount', 'PixelSum','AvgPixelValueNoDots_B', 'AvgPixelValueNoDots_G', 'AvgPixelValueNoDots_R'])\n",
    "for frame_folder in glob.glob(\"data/\"+gene_type+\"/*\"):\n",
    "    for video in  glob.glob(frame_folder+\"/*\"):\n",
    "        calculate_mean_pixels_with_tracking_no_sub(video,output_csv_path,min_max_brightness[stage_counter],thresh,\"result/\"+gene_type+\"/avg_green_dot_no_sub\")\n",
    "    stage_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748bcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
